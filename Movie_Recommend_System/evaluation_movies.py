# evaluate_performance.py
import sys
import os
from pathlib import Path

# Th√™m ƒë∆∞·ªùng d·∫´n project v√†o PYTHONPATH
current_dir = Path(__file__).parent.absolute()
sys.path.append(str(current_dir / "backend")) 
import json
import time
import pandas as pd
from datetime import datetime
from backend.app.services.top5movie_service import recommend_movies

def load_test_cases(file_path):
    """ƒê·ªçc test cases t·ª´ file JSON"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file test cases: {str(e)}")
        return []

def calculate_metrics(recommended, expected):
    """T√≠nh to√°n precision v√† recall"""
    relevant = len(set([m["title"] for m in recommended]) & set(expected))
    precision = relevant / len(recommended) if recommended else 0
    recall = relevant / len(expected) if expected else 0
    return precision, recall

def run_evaluation(test_cases):
    """Ch·∫°y ƒë√°nh gi√° tr√™n t·∫•t c·∫£ test cases"""
    results = []
    
    for idx, case in enumerate(test_cases):
        start_time = time.time()
        
        try:
            # G·ªçi logic recommendation
            recommendations = recommend_movies(case["input"])
            
            # T√≠nh to√°n th·ªùi gian v√† metrics
            response_time = time.time() - start_time
            precision, recall = calculate_metrics(recommendations, case.get("expected_movies", []))
            
            results.append({
                "test_case": case["input"],
                "precision": round(precision, 2),
                "recall": round(recall, 2),
                "response_time": round(response_time, 2),
                "recommended": [m["title"] for m in recommendations],
                "expected": case.get("expected_movies", [])
            })
            
        except Exception as e:
            print(f"L·ªói v·ªõi test case '{case['input']}': {str(e)}")
            results.append({
                "test_case": case["input"],
                "error": str(e)
            })
            
        # Progress reporting
        if (idx + 1) % 5 == 0:
            print(f"ƒê√£ x·ª≠ l√Ω {idx+1}/{len(test_cases)} test cases")
    
    return results

def generate_report(results):
    """T·∫°o b√°o c√°o chi ti·∫øt"""
    # T·∫°o DataFrame
    df = pd.DataFrame(results)
    
    # T√≠nh to√°n metrics t·ªïng h·ª£p
    summary = {
        "total_cases": len(results),
        "success_rate": len([r for r in results if "error" not in r]) / len(results),
        "avg_precision": df["precision"].mean(),
        "avg_recall": df["recall"].mean(),
        "avg_response_time": df["response_time"].mean()
    }
    
    # Xu·∫•t file
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_filename = f"evaluation_report_{timestamp}.csv"
    df.to_csv(report_filename, index=False)
    
    # In k·∫øt qu·∫£
    print("\n=== K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ===")
    print(f"T·ªïng s·ªë test cases: {summary['total_cases']}")
    print(f"T·ª∑ l·ªá th√†nh c√¥ng: {summary['success_rate']:.1%}")
    print(f"Precision trung b√¨nh: {summary['avg_precision']:.2f}")
    print(f"Recall trung b√¨nh: {summary['avg_recall']:.2f}")
    print(f"Th·ªùi gian ph·∫£n h·ªìi TB: {summary['avg_response_time']:.2f}s")
    print(f"\nB√°o c√°o ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {report_filename}")

if __name__ == "__main__":
    # C·∫•u h√¨nh
    TEST_CASE_PATH = "C:\\Users\\Admin\\Downloads\\test_cases (1).json"  # C·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø"
    
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh ƒë√°nh gi√° hi·ªáu nƒÉng...")
    
    # Th·ª±c hi·ªán ƒë√°nh gi√°
    test_cases = load_test_cases(TEST_CASE_PATH)
    
    if not test_cases:
        print("‚ùå Kh√¥ng th·ªÉ ƒë·ªçc test cases. Vui l√≤ng ki·ªÉm tra file v√† ƒë·ªãnh d·∫°ng.")
        exit(1)
        
    results = run_evaluation(test_cases)
    generate_report(results)
    
    print("‚úÖ ƒê√°nh gi√° ho√†n t·∫•t!")